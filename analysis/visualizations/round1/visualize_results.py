"""
Visualization script for Nexiom Text Game analysis results
Creates plots and charts from the CSV files generated by analyze_game_data.py
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 8)

def load_data():
    """Load analysis results from CSV files"""
    try:
        # Try results folder first
        comprehension_df = pd.read_csv('results/comprehension_analysis.csv')
        exploration_df = pd.read_csv('results/exploration_analysis.csv')
        rule_df = pd.read_csv('results/rule_understanding_analysis.csv')
        return comprehension_df, exploration_df, rule_df
    except FileNotFoundError:
        # Fall back to current directory
        try:
            comprehension_df = pd.read_csv('comprehension_analysis.csv')
            exploration_df = pd.read_csv('exploration_analysis.csv')
            rule_df = pd.read_csv('rule_understanding_analysis.csv')
            return comprehension_df, exploration_df, rule_df
        except FileNotFoundError as e:
            print(f"Error: {e}")
            print("Please run analyze_from_json.py first to generate CSV files.")
            return None, None, None


def plot_comprehension_correctness(df):
    """Plot comprehension phase correctness"""
    if df is None or len(df) == 0:
        print("No comprehension data to plot")
        return
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
    
    # Practice question correctness
    ax1 = axes[0]
    practice_counts = df['is_correct_practice'].value_counts().sort_index()
    
    # Map colors and labels dynamically based on what's in the data
    plot_colors = []
    labels = []
    for idx in practice_counts.index:
        if idx == False:
            plot_colors.append('#F44336')  # Red for incorrect
            labels.append('Incorrect')
        else:
            plot_colors.append('#4CAF50')  # Green for correct
            labels.append('Correct')
    
    practice_counts.plot(kind='bar', ax=ax1, color=plot_colors)
    ax1.set_title('Practice Question Correctness', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Correct', fontsize=12)
    ax1.set_ylabel('Count', fontsize=12)
    ax1.set_xticklabels(labels, rotation=0)
    
    # Add percentage labels
    total = practice_counts.sum()
    for i, v in enumerate(practice_counts):
        ax1.text(i, v + 0.5, f'{v}\n({v/total*100:.1f}%)', ha='center', va='bottom', fontweight='bold')
    
    # Full classification correctness
    ax2 = axes[1]
    full_counts = df['is_correct_full'].value_counts().sort_index()
    
    if len(full_counts) > 0:
        # Map colors and labels dynamically based on what's in the data
        plot_colors2 = []
        labels2 = []
        for idx in full_counts.index:
            if idx == False:
                plot_colors2.append('#F44336')  # Red for incorrect
                labels2.append('Incorrect')
            else:
                plot_colors2.append('#4CAF50')  # Green for correct
                labels2.append('Correct')
        
        full_counts.plot(kind='bar', ax=ax2, color=plot_colors2)
        ax2.set_title('Full Object Classification Correctness', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Correct', fontsize=12)
        ax2.set_ylabel('Count', fontsize=12)
        ax2.set_xticklabels(labels2, rotation=0)
        
        # Add percentage labels
        total = full_counts.sum()
        for i, v in enumerate(full_counts):
            ax2.text(i, v + 0.5, f'{v}\n({v/total*100:.1f}%)', ha='center', va='bottom', fontweight='bold')
    else:
        ax2.text(0.5, 0.5, 'No full classification data available', 
                ha='center', va='center', fontsize=14, transform=ax2.transAxes)
        ax2.set_title('Full Object Classification Correctness', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('comprehension_correctness.png', dpi=300, bbox_inches='tight')
    print("Saved: comprehension_correctness.png")
    plt.close()


def plot_exploration_stats(df):
    """Plot exploration statistics"""
    if df is None or len(df) == 0:
        print("No exploration data to plot")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Distribution of test counts
    ax1 = axes[0, 0]
    ax1.hist(df['total_tests'], bins=20, edgecolor='black', color='#2196F3', alpha=0.7)
    ax1.axvline(df['total_tests'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df["total_tests"].mean():.2f}')
    ax1.axvline(df['total_tests'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {df["total_tests"].median():.1f}')
    ax1.axvline(4, color='orange', linestyle='--', linewidth=2, label='Threshold: 4')
    ax1.set_title('Distribution of Test Counts', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Number of Tests', fontsize=12)
    ax1.set_ylabel('Frequency', fontsize=12)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Tests by phase
    ax2 = axes[0, 1]
    phase_data = df.groupby('phase')['total_tests'].agg(['mean', 'median', 'std'])
    x_pos = np.arange(len(phase_data))
    ax2.bar(x_pos, phase_data['mean'], yerr=phase_data['std'], capsize=5, color='#4CAF50', alpha=0.7, label='Mean')
    ax2.plot(x_pos, phase_data['median'], 'ro-', linewidth=2, markersize=8, label='Median')
    ax2.set_title('Tests by Phase', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Phase', fontsize=12)
    ax2.set_ylabel('Number of Tests', fontsize=12)
    ax2.set_xticks(x_pos)
    ax2.set_xticklabels(phase_data.index, rotation=45, ha='right')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Proportion testing more than 4 times
    ax3 = axes[1, 0]
    tested_more_than_4 = df['tested_more_than_4'].value_counts()
    colors = ['#F44336', '#4CAF50']  # Red for <=4, Green for >4
    labels = ['â‰¤4 tests', '>4 tests']
    wedges, texts, autotexts = ax3.pie(tested_more_than_4, labels=labels, autopct='%1.1f%%', 
                                         colors=colors, startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})
    ax3.set_title('Proportion Testing More Than 4 Times', fontsize=14, fontweight='bold')
    
    # 4. Tests by round (main experiment only)
    main_df = df[df['phase'] == 'main_experiment']
    if len(main_df) > 0:
        ax4 = axes[1, 1]
        round_data = main_df.groupby('round_number')['total_tests'].agg(['mean', 'std'])
        round_data = round_data.sort_index()
        x_pos = np.arange(len(round_data))
        ax4.bar(x_pos, round_data['mean'], yerr=round_data['std'], capsize=5, color='#9C27B0', alpha=0.7)
        ax4.set_title('Tests by Round (Main Experiment)', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Round Number', fontsize=12)
        ax4.set_ylabel('Average Number of Tests', fontsize=12)
        ax4.set_xticks(x_pos)
        ax4.set_xticklabels(round_data.index)
        ax4.grid(True, alpha=0.3, axis='y')
    else:
        ax4 = axes[1, 1]
        ax4.text(0.5, 0.5, 'No main experiment data', ha='center', va='center', fontsize=14)
        ax4.set_title('Tests by Round (Main Experiment)', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('exploration_statistics.png', dpi=300, bbox_inches='tight')
    print("Saved: exploration_statistics.png")
    plt.close()


def plot_rule_understanding(df):
    """Plot rule understanding statistics"""
    if df is None or len(df) == 0:
        print("No rule understanding data to plot")
        return
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Rule type correctness
    ax1 = axes[0, 0]
    rule_counts = df['rule_type_correct'].value_counts().sort_index()
    
    # Map colors and labels dynamically
    plot_colors = []
    labels = []
    for idx in rule_counts.index:
        if idx == False:
            plot_colors.append('#F44336')
            labels.append('Incorrect')
        else:
            plot_colors.append('#4CAF50')
            labels.append('Correct')
    
    rule_counts.plot(kind='bar', ax=ax1, color=plot_colors)
    ax1.set_title('Rule Type Classification Correctness', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Correct', fontsize=12)
    ax1.set_ylabel('Count', fontsize=12)
    ax1.set_xticklabels(labels, rotation=0)
    
    # Add percentage labels
    total = rule_counts.sum()
    for i, v in enumerate(rule_counts):
        ax1.text(i, v + 0.5, f'{v}\n({v/total*100:.1f}%)', ha='center', va='bottom', fontweight='bold')
    
    # 2. Blicket identification correctness
    ax2 = axes[0, 1]
    blicket_counts = df['blickets_correct'].value_counts().sort_index()
    
    # Map colors and labels dynamically
    plot_colors2 = []
    labels2 = []
    for idx in blicket_counts.index:
        if idx == False:
            plot_colors2.append('#F44336')
            labels2.append('Incorrect')
        else:
            plot_colors2.append('#4CAF50')
            labels2.append('Correct')
    
    blicket_counts.plot(kind='bar', ax=ax2, color=plot_colors2)
    ax2.set_title('Blicket Identification Correctness', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Correct', fontsize=12)
    ax2.set_ylabel('Count', fontsize=12)
    ax2.set_xticklabels(labels2, rotation=0)
    
    # Add percentage labels
    total = blicket_counts.sum()
    for i, v in enumerate(blicket_counts):
        ax2.text(i, v + 0.5, f'{v}\n({v/total*100:.1f}%)', ha='center', va='bottom', fontweight='bold')
    
    # 3. Performance by true rule type
    ax3 = axes[1, 0]
    rule_performance = df.groupby('true_rule')['rule_type_correct'].agg(['sum', 'count'])
    rule_performance['accuracy'] = rule_performance['sum'] / rule_performance['count'] * 100
    
    x_pos = np.arange(len(rule_performance))
    bars = ax3.bar(x_pos, rule_performance['accuracy'], color=['#2196F3', '#FF9800'])
    ax3.set_title('Rule Type Classification Accuracy by True Rule', fontsize=14, fontweight='bold')
    ax3.set_xlabel('True Rule Type', fontsize=12)
    ax3.set_ylabel('Accuracy (%)', fontsize=12)
    ax3.set_xticks(x_pos)
    ax3.set_xticklabels(rule_performance.index, rotation=45, ha='right')
    ax3.set_ylim(0, 100)
    ax3.axhline(50, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Chance (50%)')
    ax3.legend()
    
    # Add percentage labels on bars
    for i, bar in enumerate(bars):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height + 2,
                f'{height:.1f}%\n(n={int(rule_performance.iloc[i]["count"])})',
                ha='center', va='bottom', fontweight='bold')
    
    # 4. Combined performance
    ax4 = axes[1, 1]
    both_correct_df = df[(df['rule_type_correct'].notna()) & (df['blickets_correct'].notna())].copy()
    if len(both_correct_df) > 0:
        both_correct_df['both_correct'] = both_correct_df['rule_type_correct'] & both_correct_df['blickets_correct']
        performance_categories = pd.Series({
            'Both Correct': both_correct_df['both_correct'].sum(),
            'Rule Only': (both_correct_df['rule_type_correct'] & ~both_correct_df['blickets_correct']).sum(),
            'Blicket Only': (~both_correct_df['rule_type_correct'] & both_correct_df['blickets_correct']).sum(),
            'Both Incorrect': (~both_correct_df['rule_type_correct'] & ~both_correct_df['blickets_correct']).sum()
        })
        
        colors_pie = ['#4CAF50', '#FFC107', '#2196F3', '#F44336']
        wedges, texts, autotexts = ax4.pie(performance_categories, labels=performance_categories.index, 
                                            autopct='%1.1f%%', colors=colors_pie, startangle=90,
                                            textprops={'fontsize': 10, 'fontweight': 'bold'})
        ax4.set_title('Combined Performance Distribution', fontsize=14, fontweight='bold')
    else:
        ax4.text(0.5, 0.5, 'No combined data available', ha='center', va='center', fontsize=14)
        ax4.set_title('Combined Performance Distribution', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('rule_understanding.png', dpi=300, bbox_inches='tight')
    print("Saved: rule_understanding.png")
    plt.close()


def plot_confusion_matrix(df):
    """Plot confusion matrix for rule type classification"""
    if df is None or len(df) == 0:
        print("No rule understanding data for confusion matrix")
        return
    
    confusion_df = df[df['rule_type_correct'].notna()].copy()
    if len(confusion_df) == 0:
        print("No valid rule type data for confusion matrix")
        return
    
    confusion_matrix = pd.crosstab(
        confusion_df['true_rule'], 
        confusion_df['user_rule_type']
    )
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Count'})
    plt.title('Confusion Matrix: True Rule vs User Classification', fontsize=14, fontweight='bold')
    plt.xlabel('User Classification', fontsize=12)
    plt.ylabel('True Rule', fontsize=12)
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
    print("Saved: confusion_matrix.png")
    plt.close()


def main():
    print("="*80)
    print("NEXIOM TEXT GAME - VISUALIZATION")
    print("="*80)
    
    # Load data
    print("\nLoading analysis results...")
    comprehension_df, exploration_df, rule_df = load_data()
    
    if comprehension_df is None:
        print("\nNo data to visualize. Exiting.")
        return
    
    # Create plots
    print("\nGenerating visualizations...")
    
    print("\n1. Plotting comprehension correctness...")
    plot_comprehension_correctness(comprehension_df)
    
    print("\n2. Plotting exploration statistics...")
    plot_exploration_stats(exploration_df)
    
    print("\n3. Plotting rule understanding...")
    plot_rule_understanding(rule_df)
    
    print("\n4. Plotting confusion matrix...")
    plot_confusion_matrix(rule_df)
    
    print("\n" + "="*80)
    print("VISUALIZATION COMPLETE")
    print("="*80)
    print("\nGenerated files:")
    print("  - comprehension_correctness.png")
    print("  - exploration_statistics.png")
    print("  - rule_understanding.png")
    print("  - confusion_matrix.png")


if __name__ == "__main__":
    main()

